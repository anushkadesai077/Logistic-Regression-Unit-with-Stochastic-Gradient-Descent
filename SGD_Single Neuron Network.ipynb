{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V5pRK01wmD5"
   },
   "source": [
    "# Objective: \n",
    "Implement a single neuron neural network (no hidden layers) aka the logistic regression unit carrying out Stochastic Gradient Descent for optimziation and using Log Loss function for calculating cost and sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To perform everything asked in this, I have essentially divided this into 3 sections as follows: \n",
    "\n",
    "1. Implement single neuron neural network using SGD, sigmoid activation and log loss optimization\n",
    "2. Replace sigmoid activation with ReLU activation\n",
    "3. Add funcationality to perform L1 (Lasso) and/or L2 (Ridge) reguralization \n",
    "\n",
    "The common fucntions are defined initially. Then variation of few functions are defined in order to carry out the ReLU activation and then for L1 and L2 reguralization. In short, I have perform logistic regression for all variations and shown results. With every function I have also included a function description which describes what the function does.\n",
    "\n",
    "The dataset is loaded and then manually split into train-test data. It is important to scale the data for any model training. \n",
    "#### I have also implemented standard scaling manually using function that I wrote instead of sci-kit learn StandardScaler as we are not allowed to use the library. \n",
    "\n",
    "\n",
    "## Dataset used: \n",
    "Airline customer satisfaction dataset. I found this [dataset on kaggle](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction). The dataset contains a number of columns like seat comfort, food on flight, delay time, in0flight entertainment, leg room, on-board service, online support, baggage handling, checkin service etc which relate to a customer's score of how they found the overall experience. The prediction variable for this dataset is the \"satisfaction\" label which is 1 if the customer was satisfied and 0 if the customer was not satsified. the dataset contains almost equal number of samples for both prediction classes. The total number of rows in this dataset is almost 129,880."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1658899095713,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "XYiC8A8LV1xY",
    "outputId": "847545d7-0bc6-4972-8657-b920208e2ebd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "np.seterr(all=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjsGN9ElzZDY"
   },
   "source": [
    "Loading the training and testing data using the function I created. \n",
    "\n",
    "Note: I will suffle the dataset, this ensures that each time you run this code, it creates a random dataset for training and testing.\n",
    "#### As a result each time you will need to tune your paramters (learning rate, iterations, lambda for regularization) for optimal solution. A set of these parameters that got a satisfactory test accuracy will not necessarily result is satisfactory results the next time. You will need to fine tune it yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1658899095714,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "szvQ7z2T1BPa"
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    ''' This function loads the dataset and performs necessary pre-processing for the \n",
    "    logistic regression neural network  '''\n",
    "\n",
    "    df = pd.read_csv('airline_data.csv')\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    airline_train_data = df[:int(len(df)*80/100)]   #80% of the data as train data\n",
    "    airline_test_data = df[int(len(df)*80/100):]    #20% of the data for testing\n",
    "\n",
    "    X_train = airline_train_data.iloc[:,1:]\n",
    "    X_test = airline_test_data.iloc[:,1:]\n",
    "    y_train = airline_train_data.iloc[:,0].to_numpy()\n",
    "    y_test = airline_test_data.iloc[:,0].to_numpy()\n",
    "\n",
    "    for col in X_train:\n",
    "        X_train[col] = (X_train[col] - np.average(X_train[col])) / (np.std(X_train[col]))   #standardizing train data\n",
    "\n",
    "    for col in X_test:\n",
    "        X_test[col] = (X_test[col] - np.average(X_test[col])) / (np.std(X_test[col]))   #standardizing the test data\n",
    "\n",
    "    X_train = X_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "\n",
    "    print(\"Data loaded successfully after pre-processing\")\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfsG7E-zBGaK"
   },
   "source": [
    " # 1. Implement single neuron neural network using sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1658899095715,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "g97UTlejDmHx"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(size):\n",
    "    ''' This function is used to initiziale the logisitc regression praremters W (weights) and b (bias) '''\n",
    "\n",
    "    W = np.zeros_like(size)\n",
    "    b = 0\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1658899095716,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "QqfM18FSBcGS"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' This function computes and return the sigmoid activation. It always returns a value (probability) \n",
    "    between 0 to 1 which can be used to predict the class '''\n",
    "\n",
    "    return 1/(1+np.exp(-z))\n",
    "  \n",
    "def logloss(y_true, y_pred):\n",
    "    ''' This function computes the cost function for logisitc regression, here I am using the \n",
    "    log loss function formula aka the binary cross entropy loss '''\n",
    "\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        loss_sum += (y_true[i] * np.log(y_pred[i])) + ((1 - y_true[i]) * np.log(1 - y_pred[i]))\n",
    "    cost = loss_sum * (-1 / len(y_true))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def gradients(x, y, W, b):\n",
    "    ''' This function is used to compute gradient/partial derivative with respect to W (weight) and b (bias) '''\n",
    "    \n",
    "    y_pred = sigmoid(np.dot(W, x) + b)\n",
    "    dW = (y_pred - y) * x\n",
    "    db = y_pred - y\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1658899095716,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "dT4ZUgKB6SuY"
   },
   "outputs": [],
   "source": [
    "def model_train(X_train, y_train, X_test, y_test, iterations, learning_rate):\n",
    "    ''' This function implements logistic regression using stochastic gradient descent (SGD) where gradients \n",
    "    are updated using one sample at a time. \n",
    "    This is used to train the model and it returns the weights and bias along with the train and test \n",
    "    losses which are computed using log loss function '''\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    W, b = initialize_parameters(X_train[0])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        train_pred = []\n",
    "        test_pred = []\n",
    "\n",
    "        for j in range(len(X_train)):   #sending one sample at a time as I am implementing Stochastic Gradient Descent (SGD)\n",
    "            dW, db = gradients(X_train[i], y_train[i], W, b)\n",
    "            W = W - (learning_rate * dW)    #updating the weight and bias using the partial derivatives as this is SGD\n",
    "            b = b - (learning_rate * db)\n",
    "        \n",
    "        for val in range(len(X_train)):\n",
    "            train_pred.append(sigmoid(np.dot(W, X_train[val]) + b))   #compute the predictions for this iteration\n",
    "\n",
    "        for val in range(len(X_test)):\n",
    "            test_pred.append(sigmoid(np.dot(W, X_test[val]) + b))   #compute the test predictions for this iteration\n",
    "            \n",
    "        loss_train = logloss(y_train, train_pred)   #calculate the training loss using the W and b values computed for this iteration\n",
    "        loss_test = logloss(y_test, test_pred)    #calculate the test loss using the W and b values computed for this iteration\n",
    "        train_loss.append(loss_train)\n",
    "        test_loss.append(loss_test)\n",
    "        \n",
    "    return W, b, train_loss, test_loss     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1658899095717,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "BHpQw2n6DmKN"
   },
   "outputs": [],
   "source": [
    "def model_predict(W, b, X):\n",
    "    ''' This function outputs the predicted class for each sample of X using the weight and bais of trained model '''\n",
    "\n",
    "    y_predict = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        z = np.dot(W, X[i]) + b\n",
    "        y_prob = sigmoid(z)   #sigmoid is used for prediction in the output layer since it returns a probability between 0 to 1 so we can define a good threshold of 0.5\n",
    "        if y_prob >= 0.5:\n",
    "            y_predict.append(1)\n",
    "        else:\n",
    "            y_predict.append(0)\n",
    "\n",
    "    return np.array(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1658899095718,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "wIo9CSWETSsl"
   },
   "outputs": [],
   "source": [
    "def model_evaluate(y_true, y_pred):\n",
    "    ''' This function is used to cacluate the accuracy anf F1 score of the model on the predicted set by \n",
    "    comparing it with the actual labels '''\n",
    "\n",
    "    TP = 0    #true positives\n",
    "    TN = 0    #true negatives\n",
    "    FP = 0    #false positives\n",
    "    FN = 0    #false negatives\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    F1_score = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1 and y_true[i] == y_pred[i]:\n",
    "          TP += 1\n",
    "        if y_true[i] == 0 and y_true[i] == y_pred[i]:\n",
    "          TN +=1\n",
    "        if y_true[i] == 1 and y_true[i] != y_pred[i]:\n",
    "          FN += 1\n",
    "        if y_true[i] == 0 and y_true[i] != y_pred[i]:\n",
    "          FP += 1\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return accuracy, F1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4rDibNivYLE"
   },
   "source": [
    "### All the functions are now defined.\n",
    "\n",
    "Lets load the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1658899096191,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "_v5cevoe_Ygk",
    "outputId": "02efbfb7-59b3-49a5-8c0b-4aaef37670fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully after pre-processing\n",
      "X_train shape:  (103904, 19)\n",
      "y_train shape:  (103904,)\n",
      "X_test shape:  (25976, 19)\n",
      "y_test shape:  (25976,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_dataset()\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mFhHVPW5eFc"
   },
   "source": [
    "### Next, I need to train the model. In order to train a single neuron neural network we also require the learning rate and number of iterations.\n",
    "\n",
    "I am using a lower number for iterations since I have implemented SGD in this which already updates the weights and bias terms for each encountered sample, hence finding the gradient minima faster.\n",
    "\n",
    "<i>Note: the following cell takes sometime to finish running</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64305,
     "status": "ok",
     "timestamp": 1658899160494,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "jVNgTZAzxqWN",
    "outputId": "a094d4ed-fa06-4bf8-9ceb-5efcbb225bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  [0.8337196979650416, 1.3657247072545387, 1.1082005793638963, 1.159935103862531, 1.0017971665083492, 1.0151640336266534, 1.0260680329955867, 1.0455187732467373, 1.0574543574867152, 1.057663258456708, 1.1947403883912504, 1.179149693761852, 1.096188435174614, 1.0972451665123488, 1.2458559563752685, 1.2296034663138293, 1.1479517616219845, 1.2434388300627637, 1.2439570432611002, 1.2253985663188705]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "iterations = 20  \n",
    "\n",
    "W, b, train_loss, test_loss = model_train(X_train, y_train, X_test, y_test, iterations, learning_rate)\n",
    "\n",
    "#print('Train loss: ', train_loss)\n",
    "print('Test loss: ', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUw-13omwGgV"
   },
   "source": [
    "The model is trained and we have our optimized parameters: Weights and Bais. I will use these now to predict results for the train and test set. The test set accuracy is provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 795,
     "status": "ok",
     "timestamp": 1658899161277,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "EhKDjIYkSpFg",
    "outputId": "f9f13708-eed5-4ca6-b5c7-b61a837db1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.7507314444102248\n",
      "Test F1 score:  0.7635566916194997\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, train_f1_score = model_evaluate(y_train, model_predict(W, b, X_train))\n",
    "test_accuracy, test_f1_score = model_evaluate(y_test, model_predict(W, b, X_test))\n",
    "\n",
    "#print('Train Accuracy: ', train_accuracy)\n",
    "#print('Train F1 score: ', train_f1_score)\n",
    "print('Test Accuracy: ', test_accuracy)\n",
    "print('Test F1 score: ', test_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbY4S7jkAUDQ"
   },
   "source": [
    "### Model accuracy for single neuron neural network using sigmoid activation: 75.0%\n",
    "### Model F1 score for single neuron neural network using sigmoid activation: 0.763"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFyyV5HaAhDj"
   },
   "source": [
    "# 2. Change the model to now use ReLU activation\n",
    "\n",
    "This is the 2nd part of the assignment where I now use ReLU activation to train the model and get optimzied weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxXcPpBRwXxE"
   },
   "source": [
    "Note on what I did and why: \n",
    "\n",
    "ReLU function returns a value that is between 0 to infinity. Therefore, the result we get from ReLU activation can not directly be used to do predictions. In neural networks, the output layer always has a sigmoid activation function. The hidden layers usually have ReLU activation but at the end the last layer which is the output layer, the sigmoid activation is applied. This is because sigmoid returns a number between 0 to 1 which can easily be mapped to the probaility and then the 0.5 threshold can be used to predict the final output class as 0 or 1. Therefore, even in my code, I use the ReLU activation to find the optimized W and b parameters. This is where ReLU comes in. But if you see, in the end when I call the predict function, I call the function that I defined above which helps find class probabilities after apply sigmoid.\n",
    "\n",
    "Essentially ReLU is used here to find the model parameters while training the model. \n",
    "\n",
    "I define the ReLU activation function, a new gradient function ReLU and finally a new training fucntion to train parameters using ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1658899161278,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "JHfyzZe4xpwT"
   },
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    ''' This function computes and return the ReLU activation. It returns a value between 0 to infinity '''\n",
    "\n",
    "    return x * (x > 0) \n",
    "\n",
    "def gradients_with_ReLU(x, y, W, b):\n",
    "    ''' This function is used to compute gradient/partial derivative with respect to W (weight) and b (bias) \n",
    "    using ReLU as the activation function'''\n",
    "    \n",
    "    y_pred = ReLU(np.dot(W, x) + b)   #apply ReLU activation instead of sigmoid\n",
    "    dW = (y_pred - y) * x\n",
    "    db = y_pred - y\n",
    "\n",
    "    return dW, db\n",
    "\n",
    "def model_train_with_ReLU(X_train, y_train, X_test, y_test, iterations, learning_rate):\n",
    "    ''' This function implements logistic regression using stochastic gradient descent (SGD) where gradients \n",
    "    are updated using one sample at a time using ReLU activation function. \n",
    "    This is used to train the model and it returns the weights and bias along with the train and test \n",
    "    losses which are computed using log loss function '''\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    W, b = initialize_parameters(X_train[0])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        train_pred = []\n",
    "        test_pred = []\n",
    "\n",
    "        for j in range(len(X_train)):\n",
    "            dW, db = gradients_with_ReLU(X_train[i], y_train[i], W, b)\n",
    "            W = W - (learning_rate * dW)\n",
    "            b = b - (learning_rate * db)\n",
    "\n",
    "        for val in range(len(X_train)):\n",
    "            train_pred.append(sigmoid(ReLU(np.dot(W, X_train[val]) + b)))   #loss function calcualtes loss between actual y values and predicted y values\n",
    "                                                                            #Predicted y values have to be in range 0 to 1 hence you need to apply sigmoid function as well\n",
    "        \n",
    "        for val in range(len(X_test)):\n",
    "            test_pred.append(sigmoid(ReLU(np.dot(W, X_test[val]) + b)))   #loss function calcualtes loss between actual y values and predicted y values\n",
    "                                                                          #Predicted y values have to be in range 0 to 1 hence you need to apply sigmoid function as well\n",
    "\n",
    "        loss_train = logloss(y_train, train_pred)    \n",
    "        loss_test = logloss(y_test, test_pred)\n",
    "        train_loss.append(loss_train)\n",
    "        test_loss.append(loss_test)\n",
    "        \n",
    "    return W, b, train_loss, test_loss     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTyr8nFU_5D3"
   },
   "source": [
    "Now, let's train the model using this ReLU activation to get optimized weights and bais features\n",
    "\n",
    "<i>Note: the following cell takes sometime to finish running</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131716,
     "status": "ok",
     "timestamp": 1658899292990,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "x2YOIOrPnq-j",
    "outputId": "eaf82fb4-26d3-4a25-abf8-ee8b82ed1344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  [0.6931471805599296, 0.6931471805599296, 0.6931471805599296, 0.6931471805599296, 0.6401806747036672, 0.6406210009887396, 0.6411189193505127, 0.6334734620331189, 0.6334734620331189, 0.6334734620331189, 0.6482120943920854, 0.6232164146866622, 0.6027269208211457, 0.6059710713296159, 0.6036668808498614, 0.6216534445138148, 0.6185153394221701, 0.5999739226058347, 0.604246879917167, 0.5913792277656894]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "iterations = 20\n",
    "\n",
    "W, b, train_loss, test_loss = model_train_with_ReLU(X_train, y_train, X_test, y_test, iterations, learning_rate)    #find the trained model parameters using ReLU activation\n",
    "\n",
    "#print('Train loss: ', train_loss)\n",
    "print('Test loss: ', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "In-KykttARBh"
   },
   "source": [
    "Find predictions and then the test accuracy using these optimized W and b parameters that we got from just above <b><i>using the ReLU activation</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1658899293802,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "0dW6xXHbnrNH",
    "outputId": "a77f78fa-361d-47ac-9b79-8bc977e4b339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.7112334462580844\n",
      "Test F1 score:  0.760251861795634\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, train_f1_score = model_evaluate(y_train, model_predict(W, b, X_train))\n",
    "test_accuracy, test_f1_score = model_evaluate(y_test, model_predict(W, b, X_test))\n",
    "\n",
    "#print('Train Accuracy: ', train_accuracy)\n",
    "#print('Train F1 score: ', train_f1_score)\n",
    "print('Test Accuracy: ', test_accuracy)\n",
    "print('Test F1 score: ', test_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiBjk4DGA58R"
   },
   "source": [
    "### Model accuracy for single neuron neural network using ReLU activation: 71.1%\n",
    "### Model F1 score for single neuron neural network using ReLU activation: 0.760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL-nTtTlBA1q"
   },
   "source": [
    "# 3. Add funcationality to perform L1 (Lasso) and/or L2 (Ridge) reguralization\n",
    "\n",
    "I define new log loss function which also includes calculates the regularization cost. A new model training function is also defined which trains the parameters using regularization. Both these functions perform different computations depending on if you choose to apply L1 or L2 regularization. The fucntions that I have defined takes care of each case. The regularization you wish to perform is given as a parameter to the functions. \n",
    "\n",
    "What changes when regulairzation is applied is that the cost function now also includes an associated regularization cost whose formula given the log loss/binary cross entropy function is given in the function defined below. Secondly, while training the model for optimized parameters, the W and b term updation now also includes a small factor that is associated with the regulairzation we perform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1658899293803,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "wexB1mIzxp7D"
   },
   "outputs": [],
   "source": [
    "def logloss_with_regularization(y_true, y_pred, my_lambda, W, regularization):\n",
    "    ''' This function computes the cost function for our logistic regression model when there is \n",
    "    regularization performed '''\n",
    "\n",
    "\n",
    "    loss_sum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        loss_sum += (y_true[i] * np.log(y_pred[i])) + ((1 - y_true[i]) * np.log(1 - y_pred[i]))\n",
    "    cost = loss_sum * (-1 / len(y_true))\n",
    "\n",
    "    if (regularization == 'L1' or regularization == 'Lasso'):\n",
    "        regularization_cost = my_lambda * (np.sum(np.absolute(W))) / (2 * len(y_true))    #adding the regularization term's cost also for L1(Lasso) regularization\n",
    "    elif (regularization== 'L2' or regularization == 'Ridge'):\n",
    "        regularization_cost = my_lambda * (np.sum(np.square(W))) / (2 * len(y_true))     #adding the regularization term's cost also for L2(Ridge) regularization\n",
    "\n",
    "    return cost + regularization_cost   #returned cost will also have associated regularization cost\n",
    "\n",
    "def model_train_with_regularization(X_train, y_train, X_test, y_test, iterations, learning_rate, my_lambda, regularization):\n",
    "    ''' This function implements logistic regression with L1 or L2 regularization using stochastic gradient descent (SGD) \n",
    "    where gradients are updated using one sample at a time. \n",
    "    This is used to train the model and it returns the weights and bias along with the train and test \n",
    "    losses which are computed using log loss function '''\n",
    "\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    W, b = initialize_parameters(X_train[0])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        train_pred = []\n",
    "        test_pred = []\n",
    "\n",
    "        for j in range(len(X_train)):\n",
    "            dW, db = gradients(X_train[i], y_train[i], W, b)\n",
    "\n",
    "            if (regularization == 'L1' or regularization == 'Lasso'):\n",
    "                W = W - learning_rate * (dW - (2 * my_lambda/len(X_train)))   #update weight procedure when L1(Lasso) regularization is done\n",
    "            \n",
    "            elif (regularization== 'L2' or regularization == 'Ridge'):\n",
    "                W = W - learning_rate * (dW - (my_lambda * W/len(X_train)))   #update weight procedure when L2(Ridge) regularization is done\n",
    "\n",
    "            b = b - (learning_rate * db)    #no effect on bias term in regularization\n",
    "\n",
    "        for val in range(len(X_train)):\n",
    "            train_pred.append(sigmoid(np.dot(W, X_train[val]) + b))\n",
    "            \n",
    "        for val in range(len(X_test)):\n",
    "            test_pred.append(sigmoid(np.dot(W, X_test[val]) + b))\n",
    "\n",
    "        loss_train = logloss_with_regularization(y_train, train_pred, my_lambda, W, regularization)\n",
    "        loss_test = logloss_with_regularization(y_test, test_pred, my_lambda, W, regularization)\n",
    "        train_loss.append(loss_train) \n",
    "        test_loss.append(loss_test)\n",
    "        \n",
    "    return W, b, train_loss, test_loss     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEXYQlbqD--C"
   },
   "source": [
    "Now, let's train the model applying L2 (Ridge) regularization first\n",
    "\n",
    "<i>Note: the following cell takes sometime to finish running</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63600,
     "status": "ok",
     "timestamp": 1658899357398,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "bdOggSlUxqNk",
    "outputId": "b3afbdf9-37ec-4062-c69a-2724c90b7689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  [0.8337395733835279, 1.3657914355288743, 1.1083160846423246, 1.1600879061259781, 1.0019792922821218, 1.0153917621078126, 1.0263417894963283, 1.0458206872097897, 1.0578124512856142, 1.0580867698416192, 1.1951399672364162, 1.1797229947167098, 1.0967365282720478, 1.097887778127946, 1.2465792919400185, 1.2303392220459153, 1.148773868115079, 1.244419014921392, 1.2450445290508907, 1.2265475560092238]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "iterations = 20\n",
    "my_lambda = 0.1   #for L2 regularization (Ridge regularization)\n",
    "\n",
    "W, b, train_loss, test_loss = model_train_with_regularization(X_train, y_train, X_test, y_test, iterations, learning_rate, my_lambda, regularization = 'L2')\n",
    "\n",
    "#print('Train loss: ', train_loss)\n",
    "print('Test loss: ', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SDMcyeQEWzQ"
   },
   "source": [
    "Find predictions and then the test accuracy using these optimized W and b parameters that we got from just above <b><i>using L2 (Ridge) regularization</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1658899358262,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "oh2MwQHB-ciH",
    "outputId": "e5c55235-403c-4d28-ee04-b7ca2bbd37f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.7506544502617801\n",
      "Test F1 score:  0.7634836589373744\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, train_f1_score = model_evaluate(y_train, model_predict(W, b, X_train))\n",
    "test_accuracy, test_f1_score = model_evaluate(y_test, model_predict(W, b, X_test))\n",
    "\n",
    "#print('Train Accuracy: ', train_accuracy)\n",
    "#print('Train F1 score: ', train_f1_score)\n",
    "print('Test Accuracy: ', test_accuracy)\n",
    "print('Test F1 score: ', test_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYU-uWc8Evym"
   },
   "source": [
    "### Model accuracy for single neuron neural network using L2 (Ridge) regularization: 75.06%\n",
    "### Model F1 score for single neuron neural network using L2 (Ridge) regularization: 0.763"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAIaGrG5D_v8"
   },
   "source": [
    "Lastly, let's train the model applying L1 (Lasso) regularization first\n",
    "\n",
    "<i>Note: the following cell takes sometime to finish running</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65978,
     "status": "ok",
     "timestamp": 1658899424235,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "tlM4OrI7xqDu",
    "outputId": "a623f760-de34-41c6-b0e0-cd1b263f18ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  [0.83368822710536, 1.365646147525089, 1.1081395097926765, 1.159876096812088, 1.001816291816018, 1.015192303842172, 1.0260966368206896, 1.0455564591285935, 1.05749861770406, 1.0577157640087238, 1.194766964797059, 1.1791668927584973, 1.096153380261608, 1.0972026690430914, 1.2459071503553647, 1.2296224175706754, 1.1479366405055822, 1.2434642164113552, 1.2439904997397462, 1.2254455107439555, 1.1790740605015357, 1.1790804546686298, 1.215314625518764, 1.2528544534841093, 1.440054722219522, 1.2790947415440572, 1.313257369736387, 1.3834259258853452, 1.4231699874169212, 1.838530945490042, 1.84158757115995, 1.786001130506328, 1.8114172654818066, 1.654730249990512, 1.3572904805616794, 1.3565885866906027, 1.3349433249154, 1.334933380632006, 1.171928941081578, 1.377769952438555]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "iterations = 40\n",
    "my_lambda = 0.01   #for L1 regularization (Lasso regularization)\n",
    "\n",
    "W, b, train_loss, test_loss = model_train_with_regularization(X_train, y_train, X_test, y_test, iterations, learning_rate, my_lambda, regularization = 'L1')\n",
    "#print('Train loss: ', train_loss)\n",
    "print('Test loss: ', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6l_3BmoSEWHW"
   },
   "source": [
    "Find predictions and then the test accuracy using these optimized W and b parameters that we got from just above <b><i>using L1 (Lasso) regularization</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1658899424940,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "kcXlilYe-dCE",
    "outputId": "0675a14e-451e-4730-c037-d3c765430997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.7067677856482907\n",
      "Test F1 score:  0.7263517154661397\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, train_f1_score = model_evaluate(y_train, model_predict(W, b, X_train))\n",
    "test_accuracy, test_f1_score = model_evaluate(y_test, model_predict(W, b, X_test))\n",
    "\n",
    "#print('Train Accuracy: ', train_accuracy)\n",
    "#print('Train F1 score: ', train_f1_score)\n",
    "print('Test Accuracy: ', test_accuracy)\n",
    "print('Test F1 score: ', test_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8v6gyBoEm_d"
   },
   "source": [
    "### Model accuracy for single neuron neural network using L1 (Lasso) regularization: 70.06%\n",
    "### Model F1 score for single neuron neural network using L1 (Lasso) regularization: 0.726"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRIcF0LrEzBJ"
   },
   "source": [
    "As noticed, regularization does not always guarantee you a better testing accuracy. It depends on a number of things, including size of data as well as how you tune your lamdba (regularization parameter) along with learning rate and iterations. A fine tuning of these can be done, however that is out of scope of this assignment as we had to show only the process to how to perform regularization with our existing single neuron neural network code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkS1FP5GFWeR"
   },
   "source": [
    "## As seen, I have shown steps to implement the single neuron network, then to update this to use ReLU activation and then finally updated this to be able to also perform regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1658899424941,
     "user": {
      "displayName": "Vraj Mashruwala",
      "userId": "15341439260056920382"
     },
     "user_tz": 420
    },
    "id": "72WoSTL0EuoU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNCchk6ayJUS+NrpF6lRW+h",
   "collapsed_sections": [],
   "mount_file_id": "1qFPyOSxMReWXvHRXqfBPN_larqh_DzvP",
   "name": "ADS_Assignment_1.ipynb",
   "provenance": [
    {
     "file_id": "1DisX9AxsOplK6ByN9P1_OT2fRRlQhWMo",
     "timestamp": 1658799670562
    },
    {
     "file_id": "1iM_2LY5p-6Oh3k6rWQ6lteOTrVa8enwP",
     "timestamp": 1658598711990
    },
    {
     "file_id": "1LCwTkcOWxTASb_JmFrl_w-FcfPajKHpI",
     "timestamp": 1658429168763
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
